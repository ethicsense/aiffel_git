{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbc1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19eb4360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.1.3\n",
      "  Using cached matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from matplotlib==3.1.3) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from matplotlib==3.1.3) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.11 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from matplotlib==3.1.3) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from matplotlib==3.1.3) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from matplotlib==3.1.3) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.16.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ml-things 0.0.1 requires matplotlib>=3.4.0, but you have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
      "Successfully installed matplotlib-3.1.3\n"
     ]
    }
   ],
   "source": [
    "# Install transformers library.\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions.\n",
    "!pip install -q git+https://github.com/gmihaila/ml_things.git\n",
    "# Install matplotlib in proper version\n",
    "! pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f01f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (0.12.17)\n",
      "Requirement already satisfied: PyYAML in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: setproctitle in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: pathtools in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Install wandb\n",
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a65a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family='NanumBarunGothic') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16843fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "wordnet = {}\n",
    "with open(\"./kwordnet/wordnet.pickle\", \"rb\") as f:\n",
    "\twordnet = pickle.load(f)\n",
    "\n",
    "\n",
    "# 한글만 남기고 나머지는 삭제\n",
    "def get_only_hangul(line):\n",
    "\tparseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)\n",
    "\n",
    "\treturn parseText\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif len(new_words) != 0:\n",
    "\t\tsentence = ' '.join(new_words)\n",
    "\t\tnew_words = sentence.split(\" \")\n",
    "\n",
    "\telse:\n",
    "\t\tnew_words = \"\"\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynomyms = []\n",
    "\n",
    "\ttry:\n",
    "\t\tfor syn in wordnet[word]:\n",
    "\t\t\tfor s in syn:\n",
    "\t\t\t\tsynomyms.append(s)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "\treturn synomyms\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "def random_deletion(words, p):\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\t\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\tif len(new_words) >= 1:\n",
    "\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\t\tcounter += 1\n",
    "\t\telse:\n",
    "\t\t\trandom_word = \"\"\n",
    "\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "\n",
    "\n",
    "def EDA(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\tsentence = get_only_hangul(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\t# sr\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# ri\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# rs\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\t# rd\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n",
    "\trandom.shuffle(augmented_sentences)\n",
    "\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fae47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (1.1.5)\r\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from pandas) (1.21.5)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/vocrm/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc927fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class news_dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset 구성을 위한 class.\"\"\"\n",
    "\n",
    "    def __init__(self, news_dataset, labels):\n",
    "        self.news_dataset = news_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: val[idx].clone().detach() for key, val in self.news_dataset.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv 파일을 경로에 맡게 불러 옵니다.\"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    \n",
    "    print(\"Augmentation start.\")\n",
    "    \n",
    "    for i in range(len(pd_dataset)):\n",
    "        tmp = pd_dataset.iloc[i, 3]\n",
    "        tmp = EDA(tmp)\n",
    "        for sentence in tmp :\n",
    "            pd_dataset = pd_dataset.append({'class' : pd_dataset.iloc[i, 2], 'conversation' : sentence}, ignore_index=True)\n",
    "    \n",
    "    print(\"Augmentation done.\")\n",
    "    return pd_dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, max_length):\n",
    "    \"\"\"tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "    concat_entity = []\n",
    "    for temp in dataset['conversation']:\n",
    "        concat_entity.append(temp)\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False, #BERT ->  RoBERTa 120, 20 100 밥을 먹었다 [PAD] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] [1 1 1 00000000]\n",
    "    )\n",
    "    # Tokenizer : sentence -> token_id , attention mask , token_type_ids => 첫번째 문장과 두번째 문장 표시 00000000 111111111\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930a2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33methicsense\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gycksdl94/wandb/run-20220607_160007-g42hmy51</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ethicsense/huggingface3/runs/g42hmy51\" target=\"_blank\">VocClassification+augm</a></strong> to <a href=\"https://wandb.ai/ethicsense/huggingface3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation start.\n",
      "Augmentation done.\n",
      "Training Data Size : 69511\n",
      "Validation Data Size : 8688\n",
      "Testing Data Size : 8690\n",
      "\n",
      "\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 69511\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 4340\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='4340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 100/4340 08:51 < 6:23:08, 0.18 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.754600</td>\n",
       "      <td>0.462085</td>\n",
       "      <td>0.861648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.202633</td>\n",
       "      <td>0.933932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.322319</td>\n",
       "      <td>0.902970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.268195</td>\n",
       "      <td>0.923458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.298210</td>\n",
       "      <td>0.918393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8688\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./model/bert/results/checkpoint-20\n",
      "Configuration saved in ./model/bert/results/checkpoint-20/config.json\n",
      "Model weights saved in ./model/bert/results/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bert/results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8688\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./model/bert/results/checkpoint-40\n",
      "Configuration saved in ./model/bert/results/checkpoint-40/config.json\n",
      "Model weights saved in ./model/bert/results/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bert/results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8688\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./model/bert/results/checkpoint-60\n",
      "Configuration saved in ./model/bert/results/checkpoint-60/config.json\n",
      "Model weights saved in ./model/bert/results/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bert/results/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8688\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./model/bert/results/checkpoint-80\n",
      "Configuration saved in ./model/bert/results/checkpoint-80/config.json\n",
      "Model weights saved in ./model/bert/results/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bert/results/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8688\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./model/bert/results/checkpoint-100\n",
      "Configuration saved in ./model/bert/results/checkpoint-100/config.json\n",
      "Model weights saved in ./model/bert/results/checkpoint-100/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./model/bert/results/checkpoint-40 (score: 0.20263339579105377).\n",
      "Saving model checkpoint to /tmp/tmpk9muje7o\n",
      "Configuration saved in /tmp/tmpk9muje7o/config.json\n",
      "Model weights saved in /tmp/tmpk9muje7o/pytorch_model.bin\n",
      "Configuration saved in ./model/bert/best_model/config.json\n",
      "Model weights saved in ./model/bert/best_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# 모델을 실험을 해요. 기록 & 저장 -> WandB , MLflow / 실험하기 용이 -> argparse : python train.py -b 32 -lr 5e-5 + 전기세  nohup\n",
    "# config.json\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "import wandb\n",
    "\n",
    "# ------* Fix Seeds * -----------#\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"validation을 위한 metrics function\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1) #argmax([0.01 0.0001 0.1])\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "    }\n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    dict_label_to_num = {\n",
    "        '일반':0, '갈취':1, '협박':2, '직장 내 괴롭힘':3, '기타 괴롭힘':4\n",
    "    }\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label\n",
    "\n",
    "\n",
    "def train():\n",
    "    # fix a seed\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    MODEL_NAME = model_name # klue/roberta-base\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    org_dataset = load_data('./dataset/train.csv')\n",
    "    # test_dataset = load_data(\"..data/newszum_test_data.csv\")\n",
    "\n",
    "    org_label = label_to_num(org_dataset[\"class\"].values)\n",
    "    # test_label = label_to_num(test_dataset[\"class\"].values)\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_corpus = tokenized_dataset(org_dataset, tokenizer, max_len)\n",
    "    # tokenized_test = tokenized_dataset(test_dataset, tokenizer, max_len)\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    voc_dataset = news_dataset(tokenized_corpus, org_label)\n",
    "    # news_test_dataset = news_dataset(tokenized_test, test_label)\n",
    "\n",
    "    # defining split_size\n",
    "    dataset_size = len(voc_dataset)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    validation_size = int(dataset_size * 0.1)\n",
    "    test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "    # random split\n",
    "    global train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(voc_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "    print(f\"Validation Data Size : {len(valid_dataset)}\")\n",
    "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device) #a.device ,b => a*b XX\n",
    "\n",
    "    # setting model hyperparameter\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 5\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, config=model_config\n",
    "    )\n",
    "\n",
    "    print(model.config)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    ### callback & optimizer & scheduler 추가\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=3, early_stopping_threshold=0.001\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=weight_decay,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir = save_path + '/results',\n",
    "      save_total_limit = save_limit,\n",
    "      save_steps = save_step,\n",
    "      num_train_epochs = epochs,\n",
    "      learning_rate = lr,\n",
    "      per_device_train_batch_size = batch_size,\n",
    "      per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "      warmup_steps = warmup_steps,\n",
    "      weight_decay = weight_decay,\n",
    "      logging_dir = save_path + '/logs',\n",
    "      logging_steps = 10,\n",
    "      evaluation_strategy = 'steps',\n",
    "      gradient_accumulation_steps = 20,\n",
    "      eval_accumulation_steps = 20,\n",
    "      report_to = \"wandb\",\n",
    "\n",
    "      eval_steps = eval_steps,\n",
    "      load_best_model_at_end = True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset=train_dataset,  # training dataset\n",
    "      eval_dataset=valid_dataset,  # evaluation dataset\n",
    "      compute_metrics=compute_metrics,  # define metrics function\n",
    "      callbacks=[MyCallback],\n",
    "      optimizers=(\n",
    "        optimizer,\n",
    "        get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=len(train_dataset) * epochs,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_path + \"/best_model\")\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    %env WANDB_LOG_MODEL=true\n",
    "    save_path = './model/bert'\n",
    "\n",
    "    model_name = \"klue/roberta-base\"\n",
    "    seed = 42\n",
    "    max_len = 200\n",
    "    save_limit = 5\n",
    "    save_step = 20\n",
    "    epochs = 10\n",
    "    lr = 5e-4\n",
    "    batch_size = 8\n",
    "    per_device_eval_batch_size = 2\n",
    "    warmup_steps = 100\n",
    "    weight_decay = 0.005\n",
    "    logging_steps = 5\n",
    "\n",
    "    eval_steps = 20\n",
    "    load_best_model_at_end = True\n",
    "\n",
    "    # fix a seed\n",
    "    seed_everything(seed)\n",
    "    wandb.login()\n",
    "    wandb.init(project='huggingface3',\n",
    "               entity='ethicsense',\n",
    "               name='VocClassification+augm')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592892be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /home/gycksdl94/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /home/gycksdl94/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /home/gycksdl94/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /home/gycksdl94/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file ./model/bert/best_model/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./model/bert/best_model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./model/bert/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./model/bert/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|████████████████████████████████████████████████████████████████████████| 544/544 [00:29<00:00, 18.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def inference(model, tokenized_sent, device):\n",
    "    \"\"\"\n",
    "    test dataset을 DataLoader로 만들어 준 후,\n",
    "    batch_size로 나눠 model이 예측 합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
    "    model.eval()\n",
    "    output_pred = []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=data[\"input_ids\"].to(device),\n",
    "                attention_mask=data[\"attention_mask\"].to(device),\n",
    "            )\n",
    "        # print(outputs[0])\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "\n",
    "        output_pred.append(result)\n",
    "    return (np.concatenate(output_pred).tolist(),)\n",
    "\n",
    "\n",
    "def num_to_label(label):\n",
    "    \"\"\"\n",
    "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
    "    \"\"\"\n",
    "    origin_label = []\n",
    "    dict_num_to_label = {\n",
    "        0:'일반', 1:'갈취', 2:'협박', 3:'직장 내 괴롭힘', 4:'기타 괴롭힘'\n",
    "    }\n",
    "\n",
    "    for v in label[0]:\n",
    "        origin_label.append(dict_num_to_label[v])\n",
    "\n",
    "    return origin_label\n",
    "\n",
    "\n",
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "    \"\"\"\n",
    "    test dataset을 불러온 후,\n",
    "    tokenizing 합니다.\n",
    "    \"\"\"\n",
    "    test_dataset = load_data(dataset_dir)\n",
    "    test_label = list(map(int, label_to_num(test_dataset[\"class\"].values)))\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_test = tokenized_dataset(test_dataset, tokenizer, 384)\n",
    "    return tokenized_test, test_label\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    주어진 dataset csv 파일과 같은 형태일 경우 inference 가능한 코드입니다.\n",
    "    \"\"\"\n",
    "    global model, test_dataset\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # load tokenizer\n",
    "    Tokenizer_NAME = model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
    "\n",
    "    ## load my model\n",
    "    MODEL_NAME = model_dir  # model dir.\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    # model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    # ## load test datset\n",
    "    # test_dataset_dir = \"..data/newszum_test_data.csv\"\n",
    "    # test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "    # news_test_dataset = news_dataset(test_dataset, test_label)\n",
    "\n",
    "    ## predict answer\n",
    "    pred_answer = inference(model, test_dataset, device)  # model에서 class 추론\n",
    "    pred_answer = num_to_label(pred_answer)\n",
    "    # test_dataset = load_data(test_dataset_dir)\n",
    "\n",
    "    # ## make csv file with predicted answer\n",
    "    # #########################################################\n",
    "    # output = pd.DataFrame(\n",
    "    #     {\n",
    "    #         \"title\": test_dataset[\"title\"],\n",
    "    #         \"cleanBody\": test_dataset[\"cleanBody\"],\n",
    "    #         \"category\": list(test_dataset[\"category\"].values),\n",
    "    #         \"result\": pred_answer,\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    # output.to_csv(\n",
    "    #     \"./prediction/submission.csv\", index=False\n",
    "    # )  # 최종적으로 완성된 예측한 라벨 csv 파일 형태로 저장.\n",
    "    # print(\"---- Finish! ----\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # model dir\n",
    "    model = 'klue/roberta-base'\n",
    "    model_dir = save_path + '/best_model'\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de81a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a546d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
