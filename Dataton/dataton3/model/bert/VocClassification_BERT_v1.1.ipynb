{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VocClassification_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7d73694fc72454aa0ad06cf6d75c6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fb989ee60114a96abdba514a55e0e49",
              "IPY_MODEL_972556d1965a40aeb17e683d48a166da"
            ],
            "layout": "IPY_MODEL_3190189eedaa4ff59cbace4ecadcd6d4"
          }
        },
        "7fb989ee60114a96abdba514a55e0e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48e91311a6594c2c8a99e33c0c01b0e7",
            "placeholder": "​",
            "style": "IPY_MODEL_11a3751e2d864a0dbcc63072f51d8467",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "972556d1965a40aeb17e683d48a166da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0224e0a88ecb49ec957b78ac4493910d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9457bf41026c407eac720ce943f975c1",
            "value": 1
          }
        },
        "3190189eedaa4ff59cbace4ecadcd6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e91311a6594c2c8a99e33c0c01b0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a3751e2d864a0dbcc63072f51d8467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0224e0a88ecb49ec957b78ac4493910d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9457bf41026c407eac720ce943f975c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# settings, Download modules"
      ],
      "metadata": {
        "id": "009Ug-vjMt09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rh9DgDd7Mq_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b2a0fc-d958-4d54-d176-94db2f75908d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "xuD0TYSGNAjv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers library.\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "# Install helper functions.\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git\n",
        "# Install matplotlib in proper version\n",
        "! pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "wcDrY_2jNAnP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "5482b5d3-3208-41fc-8d08-9f0d6342ad65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.5 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 930 kB 42.3 MB/s \n",
            "\u001b[?25h  Building wheel for ml-things (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.2\n",
            "    Uninstalling matplotlib-3.5.2:\n",
            "      Successfully uninstalled matplotlib-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ml-things 0.0.1 requires matplotlib>=3.4.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb\n",
        "! pip install wandb"
      ],
      "metadata": {
        "id": "3hAwxApYORUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d1ad33-dd2f-40a8-ee3e-724ae63aee61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=da7fe38257b0e7ad8f8ea04092ddd47343e25a6b84f11970f7cec09a066261ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ],
      "metadata": {
        "id": "RePYcCSwNAph"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_data.py"
      ],
      "metadata": {
        "id": "ZThI4qLt8lBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class news_dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset 구성을 위한 class.\"\"\"\n",
        "\n",
        "    def __init__(self, news_dataset, labels):\n",
        "        self.news_dataset = news_dataset\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: val[idx].clone().detach() for key, val in self.news_dataset.items()\n",
        "        }\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def load_data(dataset_dir):\n",
        "    \"\"\"csv 파일을 경로에 맡게 불러 옵니다.\"\"\"\n",
        "    pd_dataset = pd.read_csv(dataset_dir)\n",
        "    return pd_dataset\n",
        "\n",
        "\n",
        "def tokenized_dataset(dataset, tokenizer, max_length):\n",
        "    \"\"\"tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
        "    concat_entity = []\n",
        "    for temp in dataset['conversation']:\n",
        "        concat_entity.append(temp)\n",
        "\n",
        "    tokenized_sentences = tokenizer(\n",
        "        concat_entity,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False, #BERT ->  RoBERTa 120, 20 100 밥을 먹었다 [PAD] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] [1 1 1 00000000]\n",
        "    )\n",
        "    # Tokenizer : sentence -> token_id , attention mask , token_type_ids => 첫번째 문장과 두번째 문장 표시 00000000 111111111\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "phQF2-im8s4h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "Nfz2wHft8tn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rKx-LPXeZJ0",
        "outputId": "36841d7c-8079-45f4-ea65-b071f1390c94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33methicsense\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 실험을 해요. 기록 & 저장 -> WandB , MLflow / 실험하기 용이 -> argparse : python train.py -b 32 -lr 5e-5 + 전기세  nohup\n",
        "# config.json\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "\n",
        "import wandb\n",
        "\n",
        "# ------* Fix Seeds * -----------#\n",
        "def seed_everything(seed):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"validation을 위한 metrics function\"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1) #argmax([0.01 0.0001 0.1])\n",
        "\n",
        "    # calculate accuracy using sklearn's function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }\n",
        "\n",
        "\n",
        "def label_to_num(label):\n",
        "    dict_label_to_num = {\n",
        "        '일반':0, '갈취':1, '협박':2, '직장 내 괴롭힘':3, '기타 괴롭힘':4\n",
        "    }\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(dict_label_to_num[v])\n",
        "\n",
        "    return num_label\n",
        "\n",
        "\n",
        "def train():\n",
        "    # fix a seed\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # load model and tokenizer\n",
        "    MODEL_NAME = model_name # klue/roberta-base\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # load dataset\n",
        "    org_dataset = load_data('./drive/MyDrive/Colab_Notebooks/voc_rm/datasets/dktc/data/insert_koen/train.csv')\n",
        "    # test_dataset = load_data(\"..data/newszum_test_data.csv\")\n",
        "\n",
        "    org_label = label_to_num(org_dataset[\"class\"].values)\n",
        "    # test_label = label_to_num(test_dataset[\"class\"].values)\n",
        "\n",
        "    # tokenizing dataset\n",
        "    tokenized_corpus = tokenized_dataset(org_dataset, tokenizer, max_len)\n",
        "    # tokenized_test = tokenized_dataset(test_dataset, tokenizer, max_len)\n",
        "\n",
        "    # make dataset for pytorch.\n",
        "    voc_dataset = news_dataset(tokenized_corpus, org_label)\n",
        "    # news_test_dataset = news_dataset(tokenized_test, test_label)\n",
        "\n",
        "    # defining split_size\n",
        "    dataset_size = len(voc_dataset)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "    # random split\n",
        "    train_dataset, valid_dataset, test_dataset = random_split(voc_dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(valid_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "    print()\n",
        "    print()\n",
        "    train_dataset.__getitem__(0)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device) #a.device ,b => a*b XX\n",
        "\n",
        "    # setting model hyperparameter\n",
        "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    model_config.num_labels = 5\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, config=model_config\n",
        "    )\n",
        "\n",
        "    print(model.config)\n",
        "    model.parameters\n",
        "    model.to(device)\n",
        "\n",
        "    ### callback & optimizer & scheduler 추가\n",
        "    MyCallback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=3, early_stopping_threshold=0.001\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-08,\n",
        "        weight_decay=weight_decay,\n",
        "        amsgrad=False,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir = save_path + '/results',\n",
        "      save_total_limit = save_limit,\n",
        "      save_steps = save_step,\n",
        "      num_train_epochs = epochs,\n",
        "      learning_rate = lr,\n",
        "      per_device_train_batch_size = batch_size,\n",
        "      per_device_eval_batch_size = 2,\n",
        "      warmup_steps = warmup_steps,\n",
        "      weight_decay = weight_decay,\n",
        "      logging_dir = save_path + '/logs',\n",
        "      logging_steps = 100,\n",
        "      evaluation_strategy = 'steps',\n",
        "      gradient_accumulation_steps = 20,\n",
        "      eval_accumulation_steps = 20,\n",
        "\n",
        "      eval_steps = 500,\n",
        "      load_best_model_at_end = True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "      model = model,\n",
        "      args = training_args,\n",
        "      train_dataset=train_dataset,  # training dataset\n",
        "      eval_dataset=valid_dataset,  # evaluation dataset\n",
        "      compute_metrics=compute_metrics,  # define metrics function\n",
        "      callbacks=[MyCallback],\n",
        "      optimizers=(\n",
        "        optimizer,\n",
        "        get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=len(train_dataset) * epochs,\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # train model\n",
        "    trainer.train()\n",
        "    model.save_pretrained(save_path + \"/best_model\")\n",
        "\n",
        "def main():\n",
        "    train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_path = './drive/MyDrive/Colab_Notebooks/voc_rm/model/bert'\n",
        "\n",
        "    model_name = \"klue/roberta-base\"\n",
        "    seed = 42\n",
        "    max_len = 400\n",
        "    save_limit = 5\n",
        "    save_step = 500\n",
        "    epochs = 10\n",
        "    lr = 5e-3\n",
        "    batch_size = 4\n",
        "    per_device_eval_batch_size = 2\n",
        "    warmup_steps = 300\n",
        "    weight_decay = 0.01\n",
        "    logging_steps = 100\n",
        "\n",
        "    eval_steps = 500\n",
        "    load_best_model_at_end = True\n",
        "\n",
        "    # fix a seed\n",
        "    seed_everything(seed)\n",
        "\n",
        "    wandb.config = {\n",
        "        'epochs': 10,\n",
        "        'classes': 5,\n",
        "        'batch_size': 4,\n",
        "        'kernels': [16, 32],\n",
        "        'weight_decay': 0.01,\n",
        "        'learning_rate': 5e-3,\n",
        "        'dataset': 'DKTC',\n",
        "        'architecture': 'RoBERTa',\n",
        "        'seed': 42\n",
        "    }\n",
        "    wandb.init(project=\"huggingface\", entity=\"ethicsense\")\n",
        "    main()"
      ],
      "metadata": {
        "id": "4YfoRvAr8u6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c7d73694fc72454aa0ad06cf6d75c6f2",
            "7fb989ee60114a96abdba514a55e0e49",
            "972556d1965a40aeb17e683d48a166da",
            "3190189eedaa4ff59cbace4ecadcd6d4",
            "48e91311a6594c2c8a99e33c0c01b0e7",
            "11a3751e2d864a0dbcc63072f51d8467",
            "0224e0a88ecb49ec957b78ac4493910d",
            "9457bf41026c407eac720ce943f975c1"
          ]
        },
        "outputId": "48796b28-5f88-4b84-9a34-e29cb162f79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:3ihrvira) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7d73694fc72454aa0ad06cf6d75c6f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">polished-moon-6</strong>: <a href=\"https://wandb.ai/ethicsense/huggingface/runs/3ihrvira\" target=\"_blank\">https://wandb.ai/ethicsense/huggingface/runs/3ihrvira</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220606_093333-3ihrvira/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:3ihrvira). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220606_093820-c62pdoz2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ethicsense/huggingface/runs/c62pdoz2\" target=\"_blank\">wobbly-yogurt-7</a></strong> to <a href=\"https://wandb.ai/ethicsense/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size : 6319\n",
            "Validation Data Size : 789\n",
            "Testing Data Size : 791\n",
            "\n",
            "\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 6319\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 790\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference.py"
      ],
      "metadata": {
        "id": "atsrvdfA82y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def inference(model, tokenized_sent, device):\n",
        "    \"\"\"\n",
        "    test dataset을 DataLoader로 만들어 준 후,\n",
        "    batch_size로 나눠 model이 예측 합니다.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
        "    model.eval()\n",
        "    output_pred = []\n",
        "    for i, data in enumerate(tqdm(dataloader)):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=data[\"input_ids\"].to(device),\n",
        "                attention_mask=data[\"attention_mask\"].to(device),\n",
        "            )\n",
        "        # print(outputs[0])\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        result = np.argmax(logits, axis=-1)\n",
        "\n",
        "        output_pred.append(result)\n",
        "    return (np.concatenate(output_pred).tolist(),)\n",
        "\n",
        "\n",
        "def num_to_label(label):\n",
        "    \"\"\"\n",
        "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
        "    \"\"\"\n",
        "    origin_label = []\n",
        "    dict_num_to_label = {\n",
        "        '일반':0, '갈취':1, '협박':2, '직장 내 괴롭힘':3, '기타 괴롭힘':4\n",
        "    }\n",
        "\n",
        "    for v in label[0]:\n",
        "        origin_label.append(dict_num_to_label[v])\n",
        "\n",
        "    return origin_label\n",
        "\n",
        "\n",
        "def load_test_dataset(dataset_dir, tokenizer):\n",
        "    \"\"\"\n",
        "    test dataset을 불러온 후,\n",
        "    tokenizing 합니다.\n",
        "    \"\"\"\n",
        "    test_dataset = load_data(dataset_dir)\n",
        "    test_label = list(map(int, label_to_num(test_dataset[\"class\"].values)))\n",
        "\n",
        "    # tokenizing dataset\n",
        "    tokenized_test = tokenized_dataset(test_dataset, tokenizer, 384)\n",
        "    return tokenized_test, test_label\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    주어진 dataset csv 파일과 같은 형태일 경우 inference 가능한 코드입니다.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # load tokenizer\n",
        "    Tokenizer_NAME = model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
        "\n",
        "    ## load my model\n",
        "    MODEL_NAME = model_dir  # model dir.\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    # model.parameters\n",
        "    model.to(device)\n",
        "\n",
        "    # ## load test datset\n",
        "    # test_dataset_dir = \"..data/newszum_test_data.csv\"\n",
        "    # test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
        "    # news_test_dataset = news_dataset(test_dataset, test_label)\n",
        "\n",
        "    ## predict answer\n",
        "    pred_answer = inference(model, test_dataset, device)  # model에서 class 추론\n",
        "    pred_answer = num_to_label(pred_answer)\n",
        "    # test_dataset = load_data(test_dataset_dir)\n",
        "\n",
        "    # ## make csv file with predicted answer\n",
        "    # #########################################################\n",
        "    # output = pd.DataFrame(\n",
        "    #     {\n",
        "    #         \"title\": test_dataset[\"title\"],\n",
        "    #         \"cleanBody\": test_dataset[\"cleanBody\"],\n",
        "    #         \"category\": list(test_dataset[\"category\"].values),\n",
        "    #         \"result\": pred_answer,\n",
        "    #     }\n",
        "    # )\n",
        "\n",
        "    # output.to_csv(\n",
        "    #     \"./prediction/submission.csv\", index=False\n",
        "    # )  # 최종적으로 완성된 예측한 라벨 csv 파일 형태로 저장.\n",
        "    # print(\"---- Finish! ----\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # model dir\n",
        "    model = 'klue/roberta-base'\n",
        "    model_dir = save_path + '/best_model'\n",
        "    main()"
      ],
      "metadata": {
        "id": "0uq3vw-W83_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}