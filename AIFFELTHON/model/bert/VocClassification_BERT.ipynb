{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VocClassification_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# settings, Download modules"
      ],
      "metadata": {
        "id": "009Ug-vjMt09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rh9DgDd7Mq_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd29b19-1959-4d7a-bec7-04ca0a1f30d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "xuD0TYSGNAjv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers library.\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "# Install helper functions.\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git\n",
        "# Install matplotlib in proper version\n",
        "! pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "wcDrY_2jNAnP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "d33c96a0-9aa6-419a-ba32-c2087925405f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Using cached matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.2\n",
            "    Uninstalling matplotlib-3.5.2:\n",
            "      Successfully uninstalled matplotlib-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ml-things 0.0.1 requires matplotlib>=3.4.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb\n",
        "! pip install wandb"
      ],
      "metadata": {
        "id": "3hAwxApYORUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1e2e5a-5e37-46ce-e6da-24eb46584156"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.17)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ],
      "metadata": {
        "id": "RePYcCSwNAph"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_data.py"
      ],
      "metadata": {
        "id": "ZThI4qLt8lBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class news_dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset 구성을 위한 class.\"\"\"\n",
        "\n",
        "    def __init__(self, news_dataset, labels):\n",
        "        self.news_dataset = news_dataset\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: val[idx].clone().detach() for key, val in self.news_dataset.items()\n",
        "        }\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def load_data(dataset_dir):\n",
        "    \"\"\"csv 파일을 경로에 맡게 불러 옵니다.\"\"\"\n",
        "    pd_dataset = pd.read_csv(dataset_dir)\n",
        "    return pd_dataset\n",
        "\n",
        "\n",
        "def tokenized_dataset(dataset, tokenizer, max_length):\n",
        "    \"\"\"tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
        "    concat_entity = []\n",
        "    for e01 in dataset['conversation']:\n",
        "        temp = \"\"\n",
        "        temp = e01\n",
        "        # 밥을 굶은 아이들 [SEP] 아이들이 밥을 굶고있다... \n",
        "        concat_entity.append(temp)\n",
        "\n",
        "    tokenized_sentences = tokenizer(\n",
        "        concat_entity,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False, #BERT ->  RoBERTa 120, 20 100 밥을 먹었다 [PAD] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] [1 1 1 00000000]\n",
        "    )\n",
        "    # Tokenizer : sentence -> token_id , attention mask , token_type_ids => 첫번째 문장과 두번째 문장 표시 00000000 111111111\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "phQF2-im8s4h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "Nfz2wHft8tn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rKx-LPXeZJ0",
        "outputId": "d40b4ed9-0620-4455-e3a8-ad70f7726130"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 실험을 해요. 기록 & 저장 -> WandB , MLflow / 실험하기 용이 -> argparse : python train.py -b 32 -lr 5e-5 + 전기세  nohup\n",
        "# config.json\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "\n",
        "import wandb\n",
        "\n",
        "# ------* Fix Seeds * -----------#\n",
        "def seed_everything(seed):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"validation을 위한 metrics function\"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1) #argmax([0.01 0.0001 0.1])\n",
        "\n",
        "    # calculate accuracy using sklearn's function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }\n",
        "\n",
        "\n",
        "def label_to_num(label):\n",
        "    dict_label_to_num = {\n",
        "        '일반':0, '갈취':1, '협박':2, '직장 내 괴롭힘':3, '기타 괴롭힘':4\n",
        "    }\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(dict_label_to_num[v])\n",
        "\n",
        "    return num_label\n",
        "\n",
        "\n",
        "def train():\n",
        "    # fix a seed\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # load model and tokenizer\n",
        "    MODEL_NAME = model_name # klue/roberta-large\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # load dataset\n",
        "    train_dataset = load_data('./drive/MyDrive/Colab_Notebooks/voc_rm/datasets/dktc/data/insert_koen/train.csv')\n",
        "    # test_dataset = load_data(\"..data/newszum_test_data.csv\")\n",
        "\n",
        "    train_label = label_to_num(train_dataset[\"class\"].values)\n",
        "    # test_label = label_to_num(test_dataset[\"class\"].values)\n",
        "\n",
        "    # tokenizing dataset\n",
        "    tokenized_train = tokenized_dataset(train_dataset, tokenizer, max_len)\n",
        "    # tokenized_test = tokenized_dataset(test_dataset, tokenizer, max_len)\n",
        "\n",
        "    # make dataset for pytorch.\n",
        "    voc_dataset = news_dataset(tokenized_train, train_label)\n",
        "    # news_test_dataset = news_dataset(tokenized_test, test_label)\n",
        "\n",
        "    # defining split_size\n",
        "    dataset_size = len(voc_dataset)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "    # random split\n",
        "    train_dataset, valid_dataset, test_dataset = random_split(voc_dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(valid_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device) #a.device ,b => a*b XX\n",
        "\n",
        "    # setting model hyperparameter\n",
        "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    model_config.num_labels = 5\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, config=model_config\n",
        "    )\n",
        "\n",
        "    print(model.config)\n",
        "    model.parameters\n",
        "    model.to(device)\n",
        "\n",
        "    ### callback & optimizer & scheduler 추가\n",
        "    MyCallback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=3, early_stopping_threshold=0.001\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-08,\n",
        "        weight_decay=weight_decay,\n",
        "        amsgrad=False,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir = save_path + '/results',\n",
        "      save_total_limit = save_limit,\n",
        "      save_steps = save_step,\n",
        "      num_train_epochs = epochs,\n",
        "      learning_rate = lr,\n",
        "      per_device_train_batch_size = batch_size,\n",
        "      per_device_eval_batch_size = 2,\n",
        "      warmup_steps = warmup_steps,\n",
        "      weight_decay = weight_decay,\n",
        "      logging_dir = save_path + '/logs',\n",
        "      logging_steps = 100,\n",
        "      evaluation_strategy = 'steps',\n",
        "      gradient_accumulation_steps = 20,\n",
        "      eval_accumulation_steps = 20,\n",
        "\n",
        "      eval_steps = 500,\n",
        "      load_best_model_at_end = True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "      model = model,\n",
        "      args = training_args,\n",
        "      train_dataset=train_dataset,  # training dataset\n",
        "      eval_dataset=valid_dataset,  # evaluation dataset\n",
        "      compute_metrics=compute_metrics,  # define metrics function\n",
        "      callbacks=[MyCallback],\n",
        "      optimizers=(\n",
        "        optimizer,\n",
        "        get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=len(train_dataset) * epochs,\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # train model\n",
        "    trainer.train()\n",
        "    model.save_pretrained(save_path + \"/best_model\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_path = './drive/MyDrive/Colab_Notebooks/voc_rm/model/bert'\n",
        "\n",
        "    model_name = \"klue/roberta-large\"\n",
        "    seed = 42\n",
        "    max_len = 400\n",
        "    save_limit = 5\n",
        "    save_step = 500\n",
        "    epochs = 10\n",
        "    lr = 5e-3\n",
        "    batch_size = 4\n",
        "    per_device_eval_batch_size = 2\n",
        "    warmup_steps = 300\n",
        "    weight_decay = 0.01\n",
        "    logging_steps = 100\n",
        "\n",
        "    eval_steps = 500\n",
        "    load_best_model_at_end = True\n",
        "\n",
        "    # fix a seed\n",
        "    seed_everything(seed)\n",
        "    main()"
      ],
      "metadata": {
        "id": "4YfoRvAr8u6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb386079-853b-429f-d86a-722f8f4df868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size : 6319\n",
            "Validation Data Size : 789\n",
            "Testing Data Size : 791\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 6319\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 790\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33methicsense\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220603_133607-3a41okti</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ethicsense/huggingface/runs/3a41okti\" target=\"_blank\">./drive/MyDrive/Colab_Notebooks/voc_rm/model/bert/results</a></strong> to <a href=\"https://wandb.ai/ethicsense/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='517' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [517/790 2:44:51 < 1:27:23, 0.05 it/s, Epoch 6.53/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.559100</td>\n",
              "      <td>1.558625</td>\n",
              "      <td>0.512041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 789\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./drive/MyDrive/Colab_Notebooks/voc_rm/model/bert/results/checkpoint-500\n",
            "Configuration saved in ./drive/MyDrive/Colab_Notebooks/voc_rm/model/bert/results/checkpoint-500/config.json\n",
            "Model weights saved in ./drive/MyDrive/Colab_Notebooks/voc_rm/model/bert/results/checkpoint-500/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference.py"
      ],
      "metadata": {
        "id": "atsrvdfA82y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def inference(model, tokenized_sent, device):\n",
        "    \"\"\"\n",
        "    test dataset을 DataLoader로 만들어 준 후,\n",
        "    batch_size로 나눠 model이 예측 합니다.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
        "    model.eval()\n",
        "    output_pred = []\n",
        "    for i, data in enumerate(tqdm(dataloader)):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=data[\"input_ids\"].to(device),\n",
        "                attention_mask=data[\"attention_mask\"].to(device),\n",
        "            )\n",
        "        # print(outputs[0])\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        result = np.argmax(logits, axis=-1)\n",
        "\n",
        "        output_pred.append(result)\n",
        "    return (np.concatenate(output_pred).tolist(),)\n",
        "\n",
        "\n",
        "def num_to_label(label):\n",
        "    \"\"\"\n",
        "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
        "    \"\"\"\n",
        "    origin_label = []\n",
        "    dict_num_to_label = {\n",
        "        '일반':0, '갈취':1, '협박':2, '직장 내 괴롭힘':3, '기타 괴롭힘':4\n",
        "    }\n",
        "\n",
        "    for v in label[0]:\n",
        "        origin_label.append(dict_num_to_label[v])\n",
        "\n",
        "    return origin_label\n",
        "\n",
        "\n",
        "def load_test_dataset(dataset_dir, tokenizer):\n",
        "    \"\"\"\n",
        "    test dataset을 불러온 후,\n",
        "    tokenizing 합니다.\n",
        "    \"\"\"\n",
        "    test_dataset = load_data(dataset_dir)\n",
        "    test_label = list(map(int, label_to_num(test_dataset[\"class\"].values)))\n",
        "\n",
        "    # tokenizing dataset\n",
        "    tokenized_test = tokenized_dataset(test_dataset, tokenizer, 384)\n",
        "    return tokenized_test, test_label\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    주어진 dataset csv 파일과 같은 형태일 경우 inference 가능한 코드입니다.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # load tokenizer\n",
        "    Tokenizer_NAME = model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
        "\n",
        "    ## load my model\n",
        "    MODEL_NAME = model_dir  # model dir.\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    # model.parameters\n",
        "    model.to(device)\n",
        "\n",
        "    # ## load test datset\n",
        "    # test_dataset_dir = \"..data/newszum_test_data.csv\"\n",
        "    # test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
        "    # news_test_dataset = news_dataset(test_dataset, test_label)\n",
        "\n",
        "    ## predict answer\n",
        "    pred_answer = inference(model, test_dataset, device)  # model에서 class 추론\n",
        "    pred_answer = num_to_label(pred_answer)\n",
        "    # test_dataset = load_data(test_dataset_dir)\n",
        "\n",
        "    # ## make csv file with predicted answer\n",
        "    # #########################################################\n",
        "    # output = pd.DataFrame(\n",
        "    #     {\n",
        "    #         \"title\": test_dataset[\"title\"],\n",
        "    #         \"cleanBody\": test_dataset[\"cleanBody\"],\n",
        "    #         \"category\": list(test_dataset[\"category\"].values),\n",
        "    #         \"result\": pred_answer,\n",
        "    #     }\n",
        "    # )\n",
        "\n",
        "    # output.to_csv(\n",
        "    #     \"./prediction/submission.csv\", index=False\n",
        "    # )  # 최종적으로 완성된 예측한 라벨 csv 파일 형태로 저장.\n",
        "    # print(\"---- Finish! ----\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # model dir\n",
        "    model = 'klue/roberta-large'\n",
        "    model_dir = save_path + '/best_model'\n",
        "    main()"
      ],
      "metadata": {
        "id": "0uq3vw-W83_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}